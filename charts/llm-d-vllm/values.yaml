# yaml-language-server: $schema=values.schema.json

# Default values for llm-d-vllm chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Common parameters
# -- Override Kubernetes version
kubeVersion: ""

# -- String to partially override common.names.fullname
nameOverride: ""

# -- String to fully override common.names.fullname
fullnameOverride: ""

# -- Default Kubernetes cluster domain
clusterDomain: cluster.local

# @schema
# additionalProperties: true
# @schema
# -- Labels to add to all deployed objects
commonLabels: {}

# @schema
# additionalProperties: true
# @schema
# -- Annotations to add to all deployed objects
commonAnnotations: {}

# @schema
# items:
#   type: [string, object]
# @schema
# -- Array of extra objects to deploy with the release
extraDeploy: []

# -- Model service controller configuration
modelservice:
  # -- Toggle to deploy modelservice controller related resources
  enabled: true

  # -- Number of controller replicas
  replicas: 1

  # -- Model Service controller image
  image:
    registry: ghcr.io
    repository: llm-d/llm-d-model-service
    tag: "0.0.10"
    imagePullPolicy: "Always"
    pullSecrets: []

  # -- RBAC configuration
  rbac:
    create: true

  # -- Service Account Configuration
  serviceAccount:
    create: true
    annotations: {}
    labels: {}

  # -- Service configuration
  service:
    enabled: true
    type: "ClusterIP"
    port: 8443

  # -- vLLM container options
  vllm:
    image:
      registry: ghcr.io
      repository: llm-d/llm-d
      tag: "0.0.8"
      imagePullPolicy: "IfNotPresent"
      pullSecrets: []

    # -- Log level for vLLM
    logLevel: "INFO"

    # -- Load format for model loading
    loadFormat: ""

    # -- Additional command line arguments for vLLM
    extraArgs: []

    # -- Additional environment variables for vLLM containers
    extraEnvVars: []

  # -- Endpoint picker configuration
  epp:
    image:
      registry: ghcr.io
      repository: llm-d/llm-d-inference-scheduler
      tag: "0.0.4"
      imagePullPolicy: "Always"
      pullSecrets: []

# -- Sample application deploying a model
sampleApplication:
  # -- Enable rendering of sample application resources
  enabled: true

  # -- Model configuration
  model:
    # -- Name of the model
    modelName: "meta-llama/Llama-3.2-3B-Instruct"

    # -- Fully qualified model artifact location URI
    modelArtifactURI: "hf://meta-llama/Llama-3.2-3B-Instruct"

    # -- HF token authentication
    auth:
      hfToken:
        name: "llm-d-hf-token"
        key: "HF_TOKEN"

  # -- Prefill configuration
  prefill:
    replicas: 1
    extraArgs: []

  # -- Decode configuration
  decode:
    replicas: 1
    extraArgs: []

  # -- Resource requirements
  resources:
    limits:
      nvidia.com/gpu: "1"
    requests:
      nvidia.com/gpu: "1"

# -- Bitnami/Redis chart configuration for caching
redis:
  enabled: true
  master:
    persistence:
      enabled: true
      size: 8Gi

# -- Integration with upstream inference gateway
inferencePool:
  # -- Enable integration with upstream inferencepool chart
  enabled: false

  # -- Model server type (vllm or triton-tensorrt-llm)
  modelServerType: vllm

  # -- Target port for model servers
  targetPort: 8000

  # -- Labels to match model servers
  modelServers:
    matchLabels:
      app: llm-d-vllm
