
# llm-d-umbrella

![Version: 1.0.0](https://img.shields.io/badge/Version-1.0.0-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: 0.1](https://img.shields.io/badge/AppVersion-0.1-informational?style=flat-square)

Complete llm-d deployment using upstream inference gateway and separated vLLM components

## Maintainers

| Name | Email | Url |
| ---- | ------ | --- |
| llm-d |  | <https://github.com/llm-d/llm-d-deployer> |

## Source Code

* <https://github.com/llm-d/llm-d-deployer>

## Requirements

Kubernetes: `>= 1.30.0-0`

| Repository | Name | Version |
|------------|------|---------|
| file://../llm-d-vllm | llm-d-vllm | 1.0.0 |
| https://charts.bitnami.com/bitnami | common | 2.27.0 |
| oci://ghcr.io/kubernetes-sigs/gateway-api-inference-extension/charts | inferencepool | 0.0.0 |

## Values

| Key | Description | Type | Default |
|-----|-------------|------|---------|
| clusterDomain | Default Kubernetes cluster domain | string | `"cluster.local"` |
| commonAnnotations | Annotations to add to all deployed objects | object | `{}` |
| commonLabels | Labels to add to all deployed objects | object | `{}` |
| fullnameOverride | String to fully override common.names.fullname | string | `""` |
| gateway | Gateway API configuration (for external access) | object | `{"annotations":{},"enabled":true,"fullnameOverride":"","gatewayClassName":"istio","kGatewayParameters":{"proxyUID":""},"listeners":[{"name":"http","port":80,"protocol":"HTTP"}],"nameOverride":"","routes":[{"backendRefs":[{"group":"inference.networking.x-k8s.io","kind":"InferencePool","name":"vllm-inference-pool","port":8000}],"matches":[{"path":{"type":"PathPrefix","value":"/"}}],"name":"llm-inference"}]}` |
| inferencepool | Enable upstream inference gateway components | object | `{"enabled":true,"inferenceExtension":{"env":[],"externalProcessingPort":9002,"image":{"hub":"gcr.io/gke-ai-eco-dev","name":"epp","pullPolicy":"Always","tag":"0.3.0"},"replicas":1},"inferencePool":{"modelServerType":"vllm","modelServers":{"matchLabels":{"app.kubernetes.io/name":"llm-d-vllm","llm-d.ai/inferenceServing":"true"}},"targetPort":8000},"provider":{"name":"none"}}` |
| kubeVersion | Override Kubernetes version | string | `""` |
| llm-d-vllm.modelservice.enabled |  | bool | `true` |
| llm-d-vllm.modelservice.vllm.podLabels."app.kubernetes.io/name" |  | string | `"llm-d-vllm"` |
| llm-d-vllm.modelservice.vllm.podLabels."llm-d.ai/inferenceServing" |  | string | `"true"` |
| llm-d-vllm.redis.enabled |  | bool | `true` |
| llm-d-vllm.sampleApplication.enabled |  | bool | `true` |
| llm-d-vllm.sampleApplication.model.modelArtifactURI |  | string | `"hf://meta-llama/Llama-3.2-3B-Instruct"` |
| llm-d-vllm.sampleApplication.model.modelName |  | string | `"meta-llama/Llama-3.2-3B-Instruct"` |
| nameOverride | String to partially override common.names.fullname | string | `""` |
| vllm | Enable vLLM model serving components | object | `{"enabled":true}` |

----------------------------------------------
Autogenerated from chart metadata using [helm-docs v1.14.2](https://github.com/norwoodj/helm-docs/releases/v1.14.2)
