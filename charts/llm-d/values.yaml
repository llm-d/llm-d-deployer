# Default values for the llm-d chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Global parameters
# Global Docker image parameters
# Please, note that this will override the image parameters, including dependencies, configured to use the global value
# Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
# @default -- See below
global:
  # -- Global Docker image registry
  imageRegistry: ""

  # -- Global Docker registry secret names as an array
  # </br> E.g. `imagePullSecrets: [myRegistryKeySecretName]`
  imagePullSecrets: []

  security:
    allowInsecureImages: true
# -- Common parameters
# -- Override Kubernetes version
kubeVersion: ""

# -- String to partially override common.names.fullname
nameOverride: ""

# -- String to fully override common.names.fullname
fullnameOverride: ""

# -- Default Kubernetes cluster domain
clusterDomain: cluster.local

# -- Labels to add to all deployed objects
commonLabels: {}

# -- Annotations to add to all deployed objects
commonAnnotations: {}

# -- Array of extra objects to deploy with the release
extraDeploy: []

# -- Helm tests
test:

  # -- Enable rendering of helm test resources
  enabled: false

# -- Sample application deploying a p-d pair of specific model
# @default -- See below
sampleApplication:

  # -- Enable rendering of sample application resources
  enabled: true

  model:
    # -- Fully qualified pvc URI: pvc://<pvc-name>/<model-path>
    modelArtifactURI: pvc://llama-3.2-3b-instruct-pvc/models/meta-llama/Llama-3.2-3B-Instruct

    # # -- Fully qualified hf URI: pvc://<pvc-name>/<model-path>
    # modelArtifactURI: hf://meta-llama/Llama-3.2-3B-Instruct

    # -- Name of the model
    modelName: "Llama-3.2-3B-Instruct"

    # -- Aliases to the Model named vllm will serve with
    servedModelNames: []

    auth:
      # -- HF token auth config via k8s secret. Required if using hf:// URI or using pvc:// URI with `--download-model` in quickstart
      hfToken:
        # -- If the secret should be created or one already exists
        create: true
        # -- Name of the secret to create to store your huggingface token
        name: llm-d-hf-token
        # -- Value of the token. Do not set this but use `envsubst` in conjunction with the helm chart
        key: HF_TOKEN

  downloadModelJob:
      # -- If `.Values.sampleApplication.model.modelArtifactURI` starts with `pvc://` what huggingface repo to load onto the pvc
    hfModelID: "meta-llama/Llama-3.2-3B-Instruct"

  # -- Modify resource limits/requests available to the pods
  # -- Resource requests/limits
  # <br /> Ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#resource-requests-and-limits-of-pod-and-container
  resources:
    limits:
      nvidia.com/gpu: 1
    requests:
      # cpu: "16"
      # memory: 16Gi
      nvidia.com/gpu: 1

  # -- InferencePool port configuration
  inferencePoolPort: 8000

# -- Gateway configuration
# @default -- See below
gateway:

  # -- Deploy resources related to Gateway
  enabled: true

  # --  String to fully override gateway.fullname
  fullnameOverride: ""

  # -- String to partially override gateway.fullname
  nameOverride: ""

  # -- Gateway class that determines the backend used
  # Currently supported values: "kgateway" or "istio"
  gatewayClassName: kgateway

  # -- Additional annotations provided to the Gateway resource
  annotations: {}

  # Special parameters applied to kGateway via GatewayParameters resource
  kGatewayParameters:

    proxyUID: false

  # Set of listeners exposed via the Gateway, also propagated to the Ingress if enabled
  listeners:
    - name: default
      path: /
      port: 80
      protocol: HTTP

  # -- Gateway's service type. Ingress is only available if the service type is set to NodePort. Accepted values: ["LoadBalancer", "NodePort"]
  serviceType: NodePort

# -- Ingress configuration
# @default -- See below
ingress:

  # -- Deploy Ingress
  enabled: true

  # -- Name of the IngressClass cluster resource which defines which controller will implement the resource (e.g nginx)
  ingressClassName: ""

  # -- Additional annotations for the Ingress resource
  annotations: {}

  # -- Hostname to be used to expose the NodePort service to the inferencing gateway
  host: ""

  # -- List of additional hostnames to be covered with this ingress record (e.g. a CNAME)
  # <!-- E.g.
  # extraHosts:
  #   - name: llm-d.env.example.com
  #     path: / (Optional)
  #     pathType: Prefix (Optional)
  #     port: 7007 (Optional) -->
  extraHosts: []

  # -- Path to be used to expose the full route to access the inferencing gateway
  path: "/"

  # -- Ingress TLS parameters
  tls:

    # -- Enable TLS configuration for the host defined at `ingress.host` parameter
    enabled: false

    # -- The name to which the TLS Secret will be called
    secretName: ""

  # -- The TLS configuration for additional hostnames to be covered with this ingress record.
  # <br /> Ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
  # <!-- E.g.
  # extraTls:
  #   - hosts:
  #     - llm-d.env.example.com
  #     secretName: llm-d-env -->
  extraTls: []

# -- Model service controller configuration
# @default -- See below
modelservice:

  # -- Toggle to deploy modelservice controller related resources
  enabled: true

  # -- Enable metrics gathering via podMonitor / ServiceMonitor
  metrics:
    enabled: true

  # --  String to fully override modelservice.fullname
  fullnameOverride: ""

  # --  String to partially override modelservice.fullname
  nameOverride: ""

  # -- Number of controller replicas
  replicas: 1

  # -- Modelservice controller image, please change only if appropriate adjustments to the CRD are being made
  image:
    registry: quay.io
    repository: llm-d/llm-d-model-service
    tag: "0.0.8"
    imagePullPolicy: "Always"

  # -- Endpoint picker configuration
  # @default -- See below
  epp:
    # -- Endpoint picker image used in ModelService CR presets
    image:
      registry: quay.io
      repository: llm-d/llm-d-inference-scheduler
      tag: "0.0.1"
      imagePullPolicy: "Always"

    metrics:

      # -- Enable metrics scraping from endpoint picker service, see `modelservice.serviceMonitor` for configuration
      enabled: true
    # -- Default environment variables for endpoint picker, use `extraEnvVars` to override default behavior by defining the same variable again.
    # Ref: https://github.com/neuralmagic/gateway-api-inference-extension/tree/dev?tab=readme-ov-file#temporary-fork-configuration
    defaultEnvVars:
      - name: ENABLE_KVCACHE_AWARE_SCORER
        value: "{{ .Values.redis.enabled }}"
      - name: KVCACHE_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: KVCACHE_INDEXER_REDIS_ADDR
        value: '{{ if .Values.redis.enabled }}{{ include "redis.master.service.fullurl" . }}{{ end }}'
      - name: ENABLE_PREFIX_AWARE_SCORER
        value: "true"
      - name: PREFIX_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: ENABLE_LOAD_AWARE_SCORER
        value: "true"
      - name: LOAD_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: ENABLE_SESSION_AWARE_SCORER
        value: "true"
      - name: SESSION_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: PD_ENABLED
        value: "true"
      - name: PD_PROMPT_LEN_THRESHOLD
        value: "10"
      - name: PREFILL_ENABLE_KVCACHE_AWARE_SCORER
        value: "true"
      - name: PREFILL_KVCACHE_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: PREFILL_ENABLE_LOAD_AWARE_SCORER
        value: "true"
      - name: PREFILL_LOAD_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: PREFILL_ENABLE_PREFIX_AWARE_SCORER
        value: "true"
      - name: PREFILL_PREFIX_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: DECODE_ENABLE_KVCACHE_AWARE_SCORER
        value: "true"
      - name: DECODE_KVCACHE_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: DECODE_ENABLE_LOAD_AWARE_SCORER
        value: "true"
      - name: DECODE_LOAD_AWARE_SCORER_WEIGHT
        value: "1.0"
      - name: DECODE_ENABLE_PREFIX_AWARE_SCORER
        value: "true"
      - name: DECODE_PREFIX_AWARE_SCORER_WEIGHT
        value: "1.0"
    # -- Additional environment variables for endpoint picker
    extraEnvVars: []

  # -- Prefill options
  # @default -- See below
  prefill:

    # -- Tolerations configuration to deploy prefill pods to tainted nodes
    # @default -- See below
    tolerations:

      # -- default NVIDIA GPU toleration
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

  # -- Decode options
  # @default -- See below
  decode:

    # -- Tolerations configuration to deploy decode pods to tainted nodes
    # @default -- See below
    tolerations:

      # -- default NVIDIA GPU toleration
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

  # -- vLLM container options
  # @default -- See below
  vllm:

    # -- vLLM image used in ModelService CR presets
    image:
      registry: quay.io
      repository: "llm-d/llm-d-dev"
      tag: "vllm-nixl-0.0.6"
      imagePullPolicy: "IfNotPresent"

    metrics:

      # -- Enable metrics scraping from vllm service, see `modelservice.serviceMonitor` for configuration
      enabled: true

  # -- Routing proxy container options
  # @default -- See below
  routingProxy:
    # -- Routing proxy image used in ModelService CR presets
    image:
      registry: quay.io
      repository: llm-d/llm-d-routing-sidecar
      tag: "0.0.5"
      imagePullPolicy: "Always"

  # -- vLL sim container options
  # @default -- See below
  vllmSim:

    # -- vLLM sim image used in ModelService CR presets
    image:
      registry: quay.io
      repository: llm-d/vllm-sim
      tag: "0.0.4"
      imagePullPolicy: "IfNotPresent"

  # -- Prometheus ServiceMonitor configuration
  # <br /> Ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md
  # @default -- See below
  serviceMonitor:

    # -- Additional annotations provided to the ServiceMonitor
    annotations: {}

    # -- Additional labels provided to the ServiceMonitor
    labels: {}

    # -- ServiceMonitor endpoint port
    port: "vllm"

    # -- ServiceMonitor endpoint path
    path: "/metrics"

    # -- ServiceMonitor endpoint interval at which metrics should be scraped
    interval: "15s"

    # -- ServiceMonitor namespace selector
    namespaceSelector:
      any: false
      matchNames: []

    # -- ServiceMonitor selector matchLabels
    # </br> matchLabels must match labels on modelservice Services
    selector:
      matchLabels: {}

  # -- Annotations to add to all modelservice resources
  annotations: {}

  # -- Pod annotations for modelservice
  podAnnotations: {}

  # -- Pod labels for modelservice
  podLabels: {}

  # Model service controller settings
  service:

    # -- Toggle to deploy a Service resource for Model service controller
    enabled: true

    # -- Port number exposed from Model Service controller
    port: 8443

    # -- Service type
    type: ClusterIP

  # -- Service Account Configuration
  # @default -- See below
  serviceAccount:

    # -- Enable the creation of a ServiceAccount for Modelservice pods
    create: true

    # --  String to fully override modelservice.serviceAccountName, defaults to modelservice.fullname
    fullnameOverride: ""

    # --  String to partially override modelservice.serviceAccountName, defaults to modelservice.fullname
    nameOverride: ""

    # -- Additional custom labels to the service ServiceAccount.
    labels: {}

    # -- Additional custom annotations for the ServiceAccount.
    annotations: {}

  rbac:
    # -- Enable the creation of RBAC resources
    create: true

# -- Bitnami/Redis chart configuration
# @default -- Use sane defaults for minimal Redis deployment
redis:
  enabled: true
  architecture: standalone
  image:
    registry: quay.io
    repository: sclorg/redis-7-c9s
    tag: c9s
  master:
    kind: Deployment
    resources:
      limits:
        memory: "256Mi"
        cpu: "250m"
      requests:
        memory: "128Mi"
        cpu: "100m"
    persistence:
      enabled: true
      size: "5Gi"
    pdb:
      create: false
  networkPolicy:
    enabled: false
