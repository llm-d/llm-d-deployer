apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "modelservice.fullname" . }}-basic-gpu-with-nixl-preset
  labels:
    {{- include "common.labels.standard" . | nindent 4 }}
    app.kubernetes.io/component: modelservice
    {{- if .Values.commonLabels }}
    {{- include "common.tplvalues.render" ( dict "value" .Values.commonLabels "context" $ ) | nindent 4 }}
    {{- end }}
  annotations:
    {{- if .Values.commonAnnotations }}
    {{- include "common.tplvalues.render" ( dict "value" .Values.commonAnnotations "context" $ ) | nindent 4 }}
    {{- end }}
    {{- if .Values.modelservice.annotations }}
    {{- include "common.tplvalues.render" ( dict "value" .Values.modelservice.annotations "context" $) | nindent 4 }}
    {{- end }}
data:
  configMaps: |
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: {{ include "modelservice.fullname" . -}}-config-decoder
      data:
        lmcache-decoder-config.yaml: |
          local_cpu: False
          max_local_cpu_size: 0
          max_local_disk_size: 0
          remote_serde: NULL
          enable_nixl: True
          nixl_role: receiver
          nixl_peer_host: 0.0.0.0
          nixl_peer_port: 55555
          nixl_buffer_size: 524288
          nixl_buffer_device: "cuda"
          nixl_enable_gc: True
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: {{ include "modelservice.fullname" . -}}-config-prefiller
      data:
        lmcache-prefiller-config.yaml: |
          local_cpu: False
          max_local_cpu_size: 0
          max_local_disk_size: 0
          remote_serde: NULL
          enable_nixl: True
          nixl_role: "sender"
          nixl_peer_host: {{`"{{ .DecodeServiceName }}"`}}
          nixl_peer_port: 55555
          nixl_buffer_size: 524288
          nixl_buffer_device: "cuda"
          nixl_enable_gc: True

  decodeDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: routing-proxy
              image: {{ include "modelservice.routingProxyImage" . }}
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
              args:
                - "--port=8001"
                - "--vllm-port=8000"
              ports:
                - containerPort: 8000
                  protocol: TCP
            - name: vllm
              image: {{ include "modelservice.vllmImage" . }}
              imagePullPolicy: {{ .Values.modelservice.vllm.image.imagePullPolicy }}
              securityContext:
                allowPrivilegeEscalation: false
              args:
                - "--port"
                - "8000"
                - "--kv-transfer-config"
                - '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_consumer","kv_connector_extra_config": {"discard_partial_chunks": false, "lmcache_rpc_port": "consumer1"}}'
              env:
                - name: XDG_CACHE_HOME
                  value: /tmp
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: status.podIP
                - name: LMCACHE_DISTRIBUTED_URL
                  value: ${POD_IP}:80
                {{- if .Values.auth.hfToken.enabled }}
                - name: HF_TOKEN
                  valueFrom:
                    secretKeyRef:
                      name: {{ .Values.auth.hfToken.name }}
                      key: {{ .Values.auth.hfToken.key }}
                {{- end }}
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: LMCACHE_USE_EXPERIMENTAL
                  value: "True"
                - name: VLLM_ENABLE_V1_MULTIPROCESSING
                  value: "1"
                - name: VLLM_WORKER_MULTIPROC_METHOD
                  value: spawn
                - name: LMCACHE_CONFIG_FILE
                  value: /vllm-workspace/lmcache-decoder-config.yaml
                {{- if .Values.redis.enabled }}
                - name: LMCACHE_LOOKUP_URL
                  value: {{ include "redis.master.service.fullurl" .}}
                {{- end }}
              ports:
                - containerPort: 8001
                  protocol: TCP
                - containerPort: 55555
                  protocol: TCP
              volumeMounts:
                - name: config-decoder
                  mountPath: /vllm-workspace
                - name: model-cache
                  mountPath: /vllm-workspace/models
                - name: model-storage
                  mountPath: /cache
          volumes:
            - name: config-decoder
              configMap:
                name: {{ include "modelservice.fullname" . -}}-config-decoder
            - name: model-cache
              emptyDir:
                sizeLimit: 1Gi

  prefillDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: "routing-proxy"
              image: {{ include "modelservice.routingProxyImage" . }}
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
              args:
                - "--port=8001"
                - "--vllm-port=8000"
              ports:
                - containerPort: 8000
                  protocol: TCP
            - name: vllm
              image: {{ include "modelservice.vllmImage" . }}
              imagePullPolicy: {{ .Values.modelservice.vllm.image.imagePullPolicy }}
              securityContext:
                allowPrivilegeEscalation: false
              args:
                - "--port"
                - "8000"
                - "--kv-transfer-config"
                - '{"kv_connector":"LMCacheConnectorV1","kv_role":"kv_producer","kv_connector_extra_config": {"discard_partial_chunks": false, "lmcache_rpc_port": "producer1"}}'
              env:
                - name: XDG_CACHE_HOME
                  value: /tmp
                - name: POD_IP
                  valueFrom:
                    fieldRef:
                      apiVersion: v1
                      fieldPath: status.podIP
                - name: LMCACHE_DISTRIBUTED_URL
                  value: "${POD_IP}:80"
                {{- if .Values.auth.hfToken.enabled }}
                - name: HF_TOKEN
                  valueFrom:
                    secretKeyRef:
                      name: {{ .Values.auth.hfToken.name }}
                      key: {{ .Values.auth.hfToken.key }}
                {{- end }}
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: LMCACHE_USE_EXPERIMENTAL
                  value: "True"
                - name: VLLM_ENABLE_V1_MULTIPROCESSING
                  value: "1"
                - name: VLLM_WORKER_MULTIPROC_METHOD
                  value: spawn
                - name: LMCACHE_CONFIG_FILE
                  value: /vllm-workspace/lmcache-prefiller-config.yaml
                {{- if .Values.redis.enabled }}
                - name: LMCACHE_LOOKUP_URL
                  value: {{ include "redis.master.service.fullurl" .}}
                {{- end }}
              volumeMounts:
                - name: config-prefiller
                  mountPath: /vllm-workspace
                - name: model-cache
                  mountPath: /vllm-workspace/models
                - name: model-storage
                  mountPath: /cache
              ports:
                - containerPort: 8001
                  protocol: TCP
                - containerPort: 55555
                  protocol: TCP
          volumes:
            - name: config-prefiller
              configMap:
                name: {{ include "modelservice.fullname" . -}}-config-prefiller
            - name: model-cache
              emptyDir:
                sizeLimit: 1Gi

  decodeService: |
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        llmd.ai/gather-metrics: "true"
    spec:
      clusterIP: None
      ports:
      - name: nixl
        port: 55555
        protocol: TCP
      - name: vllm
        port: 8000
        protocol: TCP

  prefillService: |
    apiVersion: v1
    kind: Service
    metadata:
      labels:
        llmd.ai/gather-metrics: "true"
    spec:
      clusterIP: None
      ports:
      - name: nixl
        port: 55555
        protocol: TCP
      - name: vllm
        port: 8000
        protocol: TCP

  eppService: |
    apiVersion: v1
    kind: Service
    spec:
      ports:
        - port: 9002    # Needs to match the port of the eppDeployment
          protocol: TCP
          name: grpc
        - port: 9003
          protocol: TCP
          name: grpc-health
        - port: 9090
          protocol: TCP
          name: metrics
      type: NodePort # accepts "LoadBalancer" or "NodePort"

  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          serviceAccountName: endpoint-picker-sa # manually created in workaround w/ proper RBAC
          containers:
            - args:
                - --poolName
                - {{ include "modelservice.fullname" . }}
                - --poolNamespace
                - {{ .Release.Namespace }}
                - -v
                - "4"
                - --zap-encoder
                - json
                - --grpcPort
                - "9002"
                - --grpcHealthPort
                - "9003"
              env:
              {{- if .Values.redis.enabled }}
                - name: KVCACHE_INDEXER_REDIS_ADDR
                  value: {{ include "redis.master.service.fullurl" . -}}:8100
              {{- end }}
              {{- if .Values.auth.hfToken.enabled }}
                - name: HF_TOKEN
                  valueFrom:
                    secretKeyRef:
                      name: {{ .Values.auth.hfToken.name }}
                      key: {{ .Values.auth.hfToken.key }}
              {{- end }}
              image: {{ include "modelservice.eppImage" . }}
              imagePullPolicy: {{ .Values.modelservice.epp.image.imagePullPolicy}}
              # livenessProbe:
              #   failureThreshold: 3
              #   grpc:
              #     port: 9003
              #     service: "llama-32-3b-instruct-epp-service"
              #   initialDelaySeconds: 5
              #   periodSeconds: 10
              #   successThreshold: 1
              #   timeoutSeconds: 1
              # readinessProbe:
              #   failureThreshold: 3
              #   grpc:
              #     port: 9003
              #     service: "llama-32-3b-instruct-epp-service"
              #   initialDelaySeconds: 5
              #   periodSeconds: 10
              #   successThreshold: 1
              #   timeoutSeconds: 1
              name: epp
              ports:
                - name: grpc
                  containerPort: 9002
                  protocol: TCP
                - name: grpc-health
                  containerPort: 9003
                  protocol: TCP
                - name: metrics
                  containerPort: 9090
                  protocol: TCP

  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: 8000

  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel
