apiVersion: batch/v1
kind: Job
metadata:
  name: download-model
spec:
  template:
    spec:
      containers:
        - name: downloader
          image: python:3.10
          command: ["/bin/sh", "-c"]
          args:
            - mkdir -p "${MOUNT_PATH}/models/${MODEL_NAME}" && pip install huggingface_hub && export PATH="${PATH}:${HOME}/.local/bin" && huggingface-cli login --token "${HF_TOKEN}" && huggingface-cli download "${MODEL_NAME}" --local-dir "${MOUNT_PATH}/models/${MODEL_NAME}"
          env:
            - name: HF_TOKEN # positional argument, don't move, this should be index 0 (consumed by install script)
              valueFrom:
                secretKeyRef:
                  name: "llm-d-hf-token" # default value can be patched via install script
                  key: "HF_TOKEN"
            # - name: HF_HUB_DISABLE_CACHE # No caching needed, scope PVC to model size
            #   value: "1"
            - name: HF_HOME # alternatively to disabling cache, put cache dir outside PVC
              value: /tmp/huggingface
            - name: MODEL_NAME
              value: meta-llama/Llama-3.2-3B-Instruct
            - name: HOME
              value: /tmp
            - name: MOUNT_PATH
              value: /cache # MAKE SURE SAME AS BELOW
          volumeMounts:
            - name: model-cache
              mountPath: /cache # MAKE SURE SAME AS ABOVE
      restartPolicy: OnFailure
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: llama-3.2-3b-instruct-pvc # SWAP YOUR PVC HERE
